{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2f981ad",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Read this doc from Spacy\n",
    "[Tokenization](https://spacy.io/usage/linguistic-features#tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c4afed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c26d7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f700619c",
   "metadata": {},
   "source": [
    "Tokenizer is really a special process which understands the language and does not merely split the words using some splitter (.split(\" \")). We get meaningful terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e749dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr.\n",
      "Strange\n",
      "visits\n",
      "Mumbai\n",
      "and\n",
      "he\n",
      "loves\n",
      "Pav\n",
      "Bhaji\n",
      "a\n",
      "lot\n",
      "as\n",
      "it\n",
      "costs\n",
      "just\n",
      "100\n",
      "Rs\n",
      "a\n",
      "plate\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Dr. Strange visits Mumbai and he loves Pav Bhaji a lot as it costs just 100 Rs a plate.\"\n",
    "\n",
    "doc = nlp(input_text)\n",
    "\n",
    "# print tokens, tokenized by default\n",
    "for token in doc:\n",
    "    print(token)\n",
    "    \n",
    "# printing a normal python string word by word -- this will print each character!!! but above one prints tokens - smart!\n",
    "# for word in input_text:\n",
    "#     print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f26dd724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr.\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(doc[0])\n",
    "\n",
    "print(type(doc[0]) is list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0292ac5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.token.Token"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc[0]) # its a token, not a list!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a70799a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strange visits Mumbai and\n",
      "<class 'spacy.tokens.span.Span'>\n"
     ]
    }
   ],
   "source": [
    "span = doc[1:5]\n",
    "print(span)\n",
    "print(type(span))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d37785",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(doc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d4e376aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dr."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8d13fa21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[0].is_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7f812596",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[0].like_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "46f1f832",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dr.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "db07b173",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "99009e6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[15].like_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0aa483da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[15].is_currency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0d54c8",
   "metadata": {},
   "source": [
    "### Reading from a text file and finding some info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7b716e7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dayton high school, 8th grade students information\\n',\n",
       " '==================================================\\n',\n",
       " '\\n',\n",
       " 'Name\\tbirth day   \\temail\\n',\n",
       " '-----\\t------------\\t------\\n',\n",
       " 'Virat   5 June, 1882    virat@kohli.com\\n',\n",
       " 'Maria\\t12 April, 2001  maria@sharapova.com\\n',\n",
       " 'Serena  24 June, 1998   serena@williams.com \\n',\n",
       " 'Joe      1 May, 1997    joe@root.com']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('students.txt', \"r\") as f:\n",
    "    text = f.readlines()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "72995ab5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dayton high school, 8th grade students information\\n ==================================================\\n \\n Name\\tbirth day   \\temail\\n -----\\t------------\\t------\\n Virat   5 June, 1882    virat@kohli.com\\n Maria\\t12 April, 2001  maria@sharapova.com\\n Serena  24 June, 1998   serena@williams.com \\n Joe      1 May, 1997    joe@root.com'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = ' '.join(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7b7726fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of all emails from text file: ['virat@kohli.com', 'maria@sharapova.com', 'serena@williams.com', 'joe@root.com']\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(text)\n",
    "\n",
    "emails = []\n",
    "\n",
    "for token in doc:\n",
    "    if token.like_email:\n",
    "        emails.append(token.text) # do not add token as its an object, flatten it to text\n",
    "\n",
    "print('List of all emails from text file:', emails)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c547058",
   "metadata": {},
   "source": [
    "### Lets try HINDI --- Badhiya!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "78d83c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_hindi = spacy.blank('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "69009692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "नमस्ते False False\n",
      ", False False\n",
      "मैं False False\n",
      "एक True True\n",
      "उत्कृष्ट False False\n",
      "एआई False True\n",
      "इंजीनियर False False\n",
      "बनने False False\n",
      "जा False False\n",
      "रहा False False\n",
      "हूँ False False\n",
      "! False False\n"
     ]
    }
   ],
   "source": [
    "my_text = 'नमस्ते, मैं एक उत्कृष्ट एआई इंजीनियर बनने जा रहा हूँ!'\n",
    "\n",
    "doc = nlp_hindi(my_text)\n",
    "\n",
    "for token in doc:\n",
    "    print(token, token.like_num, token.is_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f627c9",
   "metadata": {},
   "source": [
    "### Customize Tokenizer Rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "25eeb0a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gimme', 'double', 'cheese', 'large', 'healthy', 'pizza']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp('gimme double cheese large healthy pizza')\n",
    "\n",
    "tokens = [token.text for token in doc]\n",
    "\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7d08cd",
   "metadata": {},
   "source": [
    "We want to customize the tokenizer process, e.g. for gimme we want two tokens give me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63ee7ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gim', 'me', 'double', 'cheese', 'large', 'healthy', 'pizza']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.symbols import ORTH\n",
    "\n",
    "# here you can not do give and me - cant change these existing words! check more...\n",
    "nlp.tokenizer.add_special_case(\"gimme\", \n",
    "                               [\n",
    "                                   {ORTH: \"gim\"},\n",
    "                                   {ORTH: \"me\"}\n",
    "                               ])\n",
    "\n",
    "doc = nlp('gimme double cheese large healthy pizza')\n",
    "\n",
    "tokens = [token.text for token in doc]\n",
    "\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70099fbe",
   "metadata": {},
   "source": [
    "What we used in this lecture is basic model `en` which only has `tokenizer` so features are limited. When we use a full fleged pipeline like `en_web_sm` etc we get `sentenzier` and other pipeline steps as well."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
