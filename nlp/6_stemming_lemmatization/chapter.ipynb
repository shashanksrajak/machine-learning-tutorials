{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a49b0c86",
   "metadata": {},
   "source": [
    "## Stemming & Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8d032b",
   "metadata": {},
   "source": [
    "Stemming and lemmatization are natural language processing techniques used to reduce words to their base or root form.\n",
    "\n",
    "- **Stemming**: It involves removing suffixes from words to obtain their root form. For example, \"running\" becomes \"run\". Stemming may produce non-existent words as it uses heuristic rules.\n",
    "\n",
    "- **Lemmatization**: It reduces words to their dictionary form (lemma) by considering the context and meaning. For example, \"running\" becomes \"run\", and \"better\" becomes \"good\". Lemmatization is more accurate but computationally intensive compared to stemming.\n",
    "\n",
    "\n",
    "Stemming is dumber than Lemma. \n",
    "example - ability becomes abil in Stemming while able in lemmatization.\n",
    "\n",
    "Spacy does not support stemming. \n",
    "NLTK supports both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0ca9d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc6b1c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "128c87e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating  |  eat\n",
      "eats  |  eat\n",
      "eat  |  eat\n",
      "ate  |  ate\n",
      "adjustable  |  adjust\n",
      "rafting  |  raft\n",
      "ability  |  abil\n",
      "meeting  |  meet\n"
     ]
    }
   ],
   "source": [
    "words = ['eating', 'eats', 'eat', 'ate', 'adjustable', 'rafting', 'ability', 'meeting']\n",
    "\n",
    "for word in words:\n",
    "    print(word, \" | \", stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7cc6c4",
   "metadata": {},
   "source": [
    "This is fast and gets the work done in most cases. But its not accurate, \n",
    "see:\n",
    " ate - ate\n",
    " ability - abil\n",
    "\n",
    " These are not correct transformations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ddb564c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating  |  eat  |  9837207709914848172\n",
      "eats  |  eat  |  9837207709914848172\n",
      "eat  |  eat  |  9837207709914848172\n",
      "ate  |  eat  |  9837207709914848172\n",
      "adjustable  |  adjustable  |  6033511944150694480\n",
      "rafting  |  rafting  |  1196139325854331\n",
      ",  |  ,  |  2593208677638477497\n",
      "ability  |  ability  |  11565809527369121409\n",
      "meeting  |  meet  |  6880656908171229526\n",
      "better  |  well  |  4525988469032889948\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm') # load a trained model for en\n",
    "\n",
    "doc = nlp(\"eating eats eat ate adjustable rafting, ability meeting better\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token, \" | \", token.lemma_, \" | \", token.lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1939d0",
   "metadata": {},
   "source": [
    "Here ate becomes eat, ability is ability. \n",
    "\n",
    "token.lemma shows the hash for the trained words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6781bdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names # lemmatizer is there in the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e89786a",
   "metadata": {},
   "source": [
    "### Customize the lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7819947b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi  |  hi\n",
      "bro  |  Whats Up\n",
      "!  |  !\n",
      "Whats'up  |  Whats Up\n",
      "?  |  ?\n"
     ]
    }
   ],
   "source": [
    "ar = nlp.get_pipe('attribute_ruler')\n",
    "\n",
    "ar.add([[{'TEXT': 'bro'}], [{'TEXT': \"Whats'up\"}]], {'LEMMA': 'Brother', 'LEMMA': 'Whats Up'})\n",
    "\n",
    "doc = nlp(\"Hi bro! Whats'up?\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token, \" | \", token.lemma_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
