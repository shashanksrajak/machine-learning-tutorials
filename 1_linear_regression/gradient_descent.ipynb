{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "It is like a superhero that helps find our Linear Regression model to find the best possible fitting line for the data with minimal cost error.\n",
    "\n",
    "$$f(x) = wx + b$$\n",
    "\n",
    "Cost function \n",
    "$$J(w,b) = \\frac{1}{2m} \\sum_{i=1}^{m} (f(x^i) - y^i)^2$$\n",
    "\n",
    "$$w = w - \\alpha \\frac{\\partial f}{\\partial w} J(w,b)$$\n",
    "$$b = b - \\alpha \\frac{\\partial f}{\\partial b} J(w,b)$$\n",
    "\n",
    "After derivating we will get these expressions for w and b\n",
    "$$w = w - \\alpha \\frac{1}{m} \\sum_{i=1}^{m} (f(x^i) - y^i)x$$\n",
    "$$b = b - \\alpha \\frac{1}{m} \\sum_{i=1}^{m} (f(x^i) - y^i)$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x, y):\n",
    "    # we will start with some judicious values for w, b and alpha\n",
    "    w = 2 # slope\n",
    "    b = 1 # intercept\n",
    "    a = 0.04 # alpha - learning rate\n",
    "    \n",
    "    m = len(x) # number of training instances\n",
    "    \n",
    "    # fig, ax = plt.subplots()\n",
    "    \n",
    "    steps =210000\n",
    "    \n",
    "    w_calc = []\n",
    "    j_calc = []\n",
    "    \n",
    "    j_prev = 0\n",
    "    for i in range(steps):\n",
    "        y_predicted = w * x + b\n",
    "        J = 1/(2 * m) * sum(val**2 for val in y_predicted - y) # cost function calculation\n",
    "        # new w and b - calculated simultaneously\n",
    "        w = w - a * (1/m * sum((y_predicted - y) * x))\n",
    "        b = b - a * (1/m * sum(y_predicted - y))\n",
    "        \n",
    "        w_calc.append(w)\n",
    "        j_calc.append(J)\n",
    "        \n",
    "        # print(f\"Values for w : {w} and b : {b} and J : {J} in iteration {i}\")\n",
    "        \n",
    "        # compare J\n",
    "        if i > 0:\n",
    "            if(math.isclose(J, j_prev)):\n",
    "                # print(\"Hola we found a good value:\", J)\n",
    "                break\n",
    "        j_prev = J\n",
    "\n",
    "    # ax.plot(w_calc, j_calc)\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient descent result 8.999763283006654 -11.998271117149091\n",
      "result from sklearn [9.] -12.000000000000021\n"
     ]
    }
   ],
   "source": [
    "# use gradient_descent\n",
    "x = np.array([2, 4, 6, 8, 10])\n",
    "y = np.array([10, 20, 40, 60, 80])\n",
    "\n",
    "w, b = gradient_descent(x, y)\n",
    "print(\"gradient descent result\", w, b)\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(pd.DataFrame(x), y)\n",
    "print(\"result from sklearn\", lin_reg.coef_, lin_reg.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Towards the minimum the slope gets smaller and hence the movement is very little. We started with any random values for w, b and alpha (although this needs to be carefully choosen), after applying gradient descent and increasing number of iterations and tweaking alpha we were able to find a good fit, comparing to sklearn result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
